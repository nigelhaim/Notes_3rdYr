
# 08/10/2023

> [!note]- [Book](Concepts%20of%20Programming%20Languages%20(10th%20Edition)%20(Sebesta,%20Robert%20W.).pdf)
> ![](Pasted%20image%2020230810133208.png)

## Spread

![](Pasted%20image%2020230810134149.png)

> [!important]- Printed learning resources
> Robert W. Sebesta, Concepts of Programming Languages. 11th Edition Terence Pratt, Design, and Implementation of Programming Languages

## Grade Computation

![](Pasted%20image%2020230810134304.png)

# 08/12/2023


> [!note]-
> we will discuss the chapters 1 - 10 of the book
> in chapter 3 we will only be discussing until 3.4 (?)
> we will also not discuss chapter 4

## Concepts of Programming Languages

### The study of programming languages

Why study them?
- increases ability to express ideas
- improved background for choosing appropriate languages
- it makes learning new languages easier
- better understand the significance of implementation
	- better implementations of languages have been done
	- e.g. c and java

> [!seealso]- TIOBE Index
> this tries to rank the top 20 languages

Programming languages by generation

| time                     | "hardcore" | l   | "spaghetti" | "structured"             | "OOP" |     |
| ------------------------ | ---------- | --- | ----------- | ------------------------ | ----- | --- |
| 1946                     | MARK I     | l   | BASIC       | `imperative` programming |       |     |
|                          | ABC        | l   |             |                          |       |     |
|                          | ENIAC      | l   |             |                          |       |     |
| Late 1980s               | UNIVAC     | l   |             |                          |       |     |
|                          | Mainframe  | l   |             |                          |       |     |
| After lady ada something | Fortran    | l   |             |                          |       |     |
|                          | Cobol      | l   |             |                          |       |     |

- programmers are "hardcore" back then because everything starts from scratch
- the mainframe computer necessitated the need to create a programming languages
- but fortran and cobol were still too knowledge specific
	- they wanted it to be easier to use (?)
- BASIC is the first prog language to be made for beginners
	- Beginners All-purpose Symbolic Instruction Code
	- called "spaghetti" because the code is unstructured
	- the `goto` statement makes everything messy
 - imperative programming turned programming into functions
	 - clumped according to their functions
- but a problem arises that certain things in programming need attributes
	- giving birth to OOP programming
	- we can also use inheritance to adopt attributes

> [!note]-
> ForTran means formula translator

### Programming Domains

prog langs have their highlights; best features

5 types
1. Scientific Applications
	- science, math, simulation, etc.
2. Business Applications
3. Artificial Intelligence
4. Systems programming
5. Web Software
	- e.g. java (but it's not enough; only for backend)

> [!warning]
> Tests will ask the strengths and the appropriate domain of programming languages and vice versa

### Language Evaluation Criteria
1. Readability
	- indentions, etc.
	- the ease with which programs can be read and understood
1. Writability
	- e.g. click and drag feature
	- the ease with which a language can be used to create programs in the program domain
1. Reliability
	- e.g. let $a = 10, b = 2, x = \frac{a}{b}$
		- `print(x)`
	- why are declarations important for variables (?)
	- would we still be able to provide a correct output without declarations
2. Costs
3. Others

# 08/17/2023

## Language Evaluation Criteria

Summary of Criteria
![](Pasted%20image%2020230817135114.png)
### Readability

> [!note] Book reference
> starts at page 8

### Writability

Differs according to the domain for which the language is designed

Simplicity and orthogonality
- It is better to have a smaller number of well-defined constructs, a small number of primitives, and a consistent set of rules for combining them.

### Cost
1. training programmers to use the language
2. Writing programs (closeness to particular applications)
3. Compiling efficiency
	- Tradeoff with execution efficiency. ..
	- e.g. importing java packages, the more lines of codes to compile, the longer it takes
	- but the longer to compile, the execution process is faster, because the flaws/exceptions are already caught
1. Executing efficiency
2. Language implementation system
	- Availability of free compilers...
3. Reliability: poor reliability leads to high costs
	- Failure in critical systems vs. non-critical systems
		- Law-suits and Business Reputation
		- critical systems are systems where even small errors in data can lead to disastrous consequences
			- e.g. nasa designing softwares for rockets
			- they might have rounded off too much significant figures to the point that the data is inaccurate
1. Maintaining programs
	- e.g. legacy systems; programs developed a long time ago

### Others
- Portability
	- The ease with which programs can be moved from one platform to another
- Generality
	- The applicability to a wide range of applications
- Well-definedness
	- The completeness and precision of the language's official definition

### Criteria and Language Design

Difficulty of implementing the various constructs and features.

May emphasize Elegance and attracting widespread use.
- e.g. indenting, use of white spaces, etc.

## Influences on Langauge Design
- Computer Architecture
	- The underlying architectural model for procedural languages is known as the von Neumann architecture
- Programming Methodologies
	- Emergence of new software development methodologies
	- Examp: OOD led to new programming paradigms and by extension, new OOPLs

### The von Neumann Architecture

Fetch-execute-cycle (on a von Neumann architecture computer):
```
initialize the program counter
repeat forever
	fetch the instruction pointed by the counter
	increment the counter
	decode the instruction
	execute the instruction
	store the result
end repeat
```

![](Pasted%20image%2020230817141721.png)

### Computer Architecture Influence

Machine architecture.
- von Neumann machine
	- basis of imperative langauges
	- functional languages cannot be applied efficiently to this architecture

### Programming Methodologies Influence
- 50s and early 60s
	- Simple applications
	- Concerns were about machine efficiency
	- UNIVAC came into picture
- Late 60s, early 70s:
	- Programming problems more complex.
		- from calculating multiple equations, it became more complex such as worldwide airline reservation systems
	- Cost of hardware reduced.
	- Cost of software development increased.
		- People efficiency became important.
	- Structured Programming Movement led to Top-Down-Stepwise Refinement.
		- Readability, Better control structures ("gotoless" programming)
- Late 70s
	- shift from procedure-oriented to data-oriented desigm
	- Data abstraction to encapsulate processing with data
		- SIMULA 67 supported these ideas but had limited support
- Middle 80s: Object-oriented programming
	- Data abstraction plus Inheritance and Dynamic method binding (polymorphism)
	- Smalltalk, Ada 95, Java, C++.
- More recently procedure oriented programming applied to concurrency
	- Ada, Java, C# have capabilities to control concurrent program units.

### Other influences on Langauge Design
- Concerns for commercial success
- Implementation difficulties
- Academic
- Computer Architecture (read about this at page 18)

# Self Study 08/19/2023

## Language Categories

> [!tldr]- Page
> 21 or 42 of 816

## Language Design Trade-Offs

> [!tldr]- Page
> 23 or 44 of 816

The language evaluation criteria in itself has contradicting items. e.g. to be more reliable, cost of execution increases. 

Language design requires choosing certain characteristics and trade offs between these criterias

## Implementation Methods

> [!tldr]- Page
> 23 or 44 of 816

### Compilation
- programs are translated into machine language
- very fast program execution

> [!info] Source language
> The programming language that a compiler translates into machine code

![](Pasted%20image%2020230819175712.png)

The lexical analyzer gathers the characters of the source program into lexical units
- lexical units are identifiers, special words, operators, punctuation symbols
- lexical analyzers ignores comments

The syntax analyzer takes the lexical units from the lexical analyzer to create **parse trees**
- these represent the syntactic structure of the program
- no actual parse trees are created in most cases, just the information (abstraction) is there

The intermediate code generator creates a program in a different language
- lower level than the high-level, source language but higher level than machine language
- looks similar to assembly lang, sometimes they actually are
- but in other cases, the intermediate is at a level somewhat higher than an assembly language

The semantic analyzer is an integral part of the intermediate code generator
- checks for errors and type errors that are difficult/impossible to detect during syntax analysis

> [!info] Optimization
> Improves programs (usually in their intermediate code version because it is easier there) by making them smaller or faster or both.
> 
> often an optional part of compilation

The code generator translates the optimized intermediate code version of the program into an equivalent machine language program

The symbol table serves as a database for the compilation process
- primary contents are the type and attriute info of each user-defined name in the program
- this information is place din the symbol table by the lexical and syntax analyzers and is used by the semantic analyzer and the code generator

> [!important]
> Although the machine langauge generated by a compiler can be executed directly on the hardware, it must nearly always be run along with some other code. e.g. programs from the operating system

Most user programs also require programs from the operating system
- e.g. the input and output

The compiler builds calls to required system programs when they are needed by the user program.

Before execution, the required programs from the OS must be found and linked to the user program
- the linking operation connects the user program to the system programs by placing the addresses of the entry points of the system programs in the calls to them in the user program

> [!info] Load module/executable image
> When the user and system code are now combined toegher

> [!info] Linking and Loading a.k.a. linking
> the process of collecting system programs and linking them to user programs
> 
> This is accomplished by a systems program called a ==linker==

other than systems programs, user programs need to be linked with other user programs as well residing in libraries. $\therefore$ the linker links a given program to system programs, but also to other user programs

> [!info] von Neumann bottleneck
> When instructions are executed faster than they can be moved to the processor for execution

### Pure Interpretation

> [!tldr]- Page
> 28 or 49 of 816

The complete opposite of the compiler implementation. programs are interpreted by another program called an interpreter without any translation.

The interpreter program acts as a software simulation fo a machine whose fetch-execute cycle deals with high-level language program statements rather than machine instructions. Providing a virtual machine for the language

this gives the advantage of allowing easy implementation of many source-level debugging operations
- since all run-time error messages can refer to source-level units, the error message can easily indicate the source line and the name of the associated variables.

> [!warning]
> However, this method has the serious disadvantage that execution is 10 to 100 times slower than in compiled systems
> 
> Mainly because of the decoding of the high-level language statements which are far more complex than machine language instructions
> 
> Additionally, regradless of how many times a statement is executed, it must be decoded everytime.
> this makes statement decoding the bottleneck o fa pure interpreter

Pure interpretation also often requires more space due to the following factors
- The source program
- the symbol table must be present during interpretation

![](Pasted%20image%2020230819184848.png)

### Hybrid Implementation Systems

![](Pasted%20image%2020230819185350.png)

Continue at page 29

# 08/24/2023

## Evolution of the Major Programming Languages

![](Pasted%20image%2020230824134437.png)

> [!note]- [Levenez](https://www.levenez.com/lang/)
> shows the history of programming languages
> also some prog langs arranged alphabetically

### Early History: First Programmers
- Jacquard loom
- Charles Babbage's analytical engine
- Ada lovelace - first programmer

Konrad Zuse
- creator of Plankalkul
- the first programming language
- his work was not fully documented until decades after

The 1940s: Von Neumann and Zuse
- defined Plankalkul (program calculus) circa 1945
- wasn't able to implement it

[](Pasted%20image%2020230824135149.png) ![](Pasted%20image%2020230824135157.png)

> [!info]
> The ENIAC was one of the first computers

### 1950s
- compiling systems
- relocatable code
- assembly languages
- Fortran 0, I, II
- AI - FLPL; IPL-I, LISP
- Business - Flowmatic & COBOL
- ALGOL

A first compiling system
- 1950 - 1953
- Grace Hopper and her team at UNIVAC
	- Developed a "compiling system" named A-0, A-I , and A-2
	- Programs were written in a type of code called pseudocode which was expanded into machine code subprograms

Progress in Generality
- 1950
- David J. Wheeler
	- Developed a method of using blocks of relocatable addresses.

Speedcode System
- Support for Floating Point Operations
- Early 1950s
- John Backus
- ![](Pasted%20image%2020230824135739.png)

First real compiler
![](Pasted%20image%2020230824135811.png)

Fortran
- A Significant Step Up — First Important High Level Language
- **For**mula **Tran**slator
- ![](Pasted%20image%2020230824135934.png)

Artifical Intelligence
- An Influence on Programming Languages
- Mid 50s
	- Interest in Al emerged
	- Natural language processing
	- Modeling human information storage and retrieval and other brain processes
	- Mechanizing certain intelligent processes such as theorem proving
	- These interests held a common need for processing symbolic data in lists that could be easily manipulated
		- linked lists vs. arrays.

AI and Fortran
- Mid 50s
- IBM
	- FLPL (Fortran List Processing Language)
		- Extension to the Fortran compiler
		- Used to construct a theorem prover for plane geometry

First AI Programming Language
IPL - Information Processing Language
- 1956
- Allen Newell, J.C. Shaw, Herbert Simon
	- Published a description of one of the first Al languages
		- IPL-I
		- Information Processing Language

The Business Domain
- 1957
- FLOW-MATIC
	- Business oriented language for the UNIVAC
	- "mathematical programs should be written in mathematical notation, data processing programs should be written in English statements" — Grace Hopper 1953
- 1959
	- COBOL 60
		- Common Business Oriented Language
		- Design Goals
			- Use English as much as possible
			- Easy to use, even at the expense of being less powerful..
			- Not be overly restricted by the problems of its implementation
		- Characteristics
			- DEFINE verb for macros
			- Records
			- 30 character names with hyphens
			- Data division and procedure division
			- Mandated by the DOD
			- Example on pages 65-67
		- Standardized versions in 1968; 1974 (subprograms with parameters), 1985, 2002...

Fortran progresses
- 1958
- Fortran II compiler
	- Bug fixes
	- Independent compilation of subprograms
		- made lengthier programs possible

ALGOL
- Algorithmic Language
- ![](Pasted%20image%2020230824141722.png)

LISP
- 1959
- John McCarthy and Marvin Minsky
	- Produced a system for list processing
	- LISP
		- A functional language
		- Dominated for a decadee
		- Originally interpretede
		- Example diagrams and program on pages 54-55.e
		- Decendents are:
			- Scheme
			- COMMON LISP
		- Related Language - ML

![](Pasted%20image%2020230824142102.png)

### 1960s
- IPL-V
- ALGOL-GO
- APL
- SNOBOL
- Fortran IV
- BASIC
- PL/1
- SIMULA
- ALGOL-68

Progress in AI
- 1960
- Newell and Tonge
	- IPL-V
		- Demonstrated that list processing was feasible and useful.
		- Actually an assembly language implemented in an interpreter with list processing instructions for the Johnniac machine

ALGOL 60
- 1960
- ALGOL 60
	- Formally described using Backus-Naur Form
	- Example on pages 61-62
	- New additions
		- Block structure concept
		- Pass by value and pass by name
		- Recursive procedures
		- Stack-dynamic arrays
	- Note that formatted 1/0 was omitted due to goal of machine independence
	- Parent
		- Fortran
	- Descendants
		- PL/I, SIMULA 67, C, Pascal, Ada, C++ and Java

![](Pasted%20image%2020230824143033.png)

APL and SNOBOL
- ==to be continued==

# 08/26/2023

## Evolution of the Major Prog Langs

APL and SNOBOL
- Kenneth Iversion at IBm
- Two early dynamic languages
- dynamic storage location

SNOBOL
- D.J. Farber, R.E. Griswold

> [!note] Refer to page 71

Fortran IV
- 1962
- most widely used PLs
- explicit type declarations for variables
- Logical If construct
- capable of passing subprograms as parameters

> [!info]- 1966
> Fortran 66 - the standardized version

BASIC
- 1963
- John Kemeny and Thomas Kurtz designed BASIC (beginner's all purpose symbolic instruction code)
- Goals
	- must be easy for non-science students to learn and use
	- must be pleasant and friendly
	- provide fast turnaround for homework
	- must allow free and private access
	- must consider user time more important than computer time
- Characteristics
	- small, non interactive
	- used through terminals
	- simgle data type - fp - numbers
	- resurgence with Visual BASIC in 90s
	- example on page 69
	- [!] overuse of `goto` statements

> [!important] Topic focus
> Focus on the features of the programming languages

### 1970s

Logic Languages
- 1975
- Philippe Roussel
	- described Prolog
	- example code on page 86

Scheme
- A functional programming language
- 1975
- MIT
- small size
- exclusive use of static scoping
- functions are first-class entities - can be values of expressions and elements of lists; assigned to variables; passed as parameters and returned as values of function applications
- simply syntax and semantics

Fortran 77
- 1978
- Character string handling
- logical loop control statements
- if else

> [!quote] Alan Perlis
> *Fortran is the "lingua franca" of the computing world..*

### 1980s

Smalltalk
- 1980
- Allan Kay who predicted computer "desktop" windowing environment
	- Developed the first language that fully supported OOP as a part of the Xerox Palo Alto Research Center (PARC) group
- objects and message passing
- example on pages 93-94
- First programming langauge that fully supported object-oriented programming

MetaLanguage
- 1980s
- Robin Milner
	- functional but supports imperative

ADA
- 1983
- for the boujees

LISP
- 1984
- COMMON LISP
- designed to combine featuers of a number of different dialects of LISP that were developed during the 70s and 80s
- Large and complex
- allows both dynamic and static scoping
- Basis is pure lisp

Miranda
- 1984
- David Turner
	- Miranda
		- Based on ML, SASL, and KRC
		- Functional, no variables, no assignment statement
		- Haskell is based on Miranda
			- But has the unique feature of lazy evaluation
				- No expression is evaluated until its value is required.

C++
- Bjorne Stroustrup at Bell Labs
	- Made the first step from C to C++ with C with Classes language int 1983
	
	- **Goals**
		- provide a langiage with classes and inheritance
		- no performance penalty — reason array index range checking was not considered
		- it could be used for every application of which C was used — so left unsafe features of C
	
	- **Progression**
		- **1980**
			- Addition of function parameter type checking and conversion
			- Classes like those of SIMULA 67 and Smalltalk
			- Derived classes, public/private access, constructors/destructors, friends
		
		- **1981**
			- In-line functions, default parameters, overloading
		
		- **1984**
			- Named C++ virtual methods, dynamic binding of method calls to method definitions, reference types
		
		- **1985**
			- First available implementation named CFront which translates C++ programs into C programs
			- Continued to evolve
				- multiple inheritance, abstract classes
				- templates which provide parameterized types and exception handling
				- 2002 — .NET

### 1990s

Fortran 90
- 1990
- Supports
	- Dynamic arryas
	- Records
	- Pointers
	- Multiple selection statement
		- falls under the category of conditional statements
	- Modules
	- Recursion
	- Obsolescent-features list
	- Dropped fixed format of code requirement
	- Fortran vs. FORTRAN
	- Convention — keywords & identifiers in uppercase

Fortran 95
- 1995
- Features
	- Adding new components to those…

Java
- was created from C++ (1985)
- 1994

### Scripting Languages
- Sh (Shell)
	- a small collection of commands interpreted as calls to system subprograms to perform utility functions with added variables, control flow statements, functions andetc
		- Ksh — David Koran ‘95
	s
	Awk
		- Al Aho
		- Brain Kernighan
		- Peter Weinberger (‘88) began as a report generation language
	
	- Tcl
	- Perl

### 2000s
- Fortran 2002
- C#

# 09/07/2023

## Chapter 3: Describing Syntax & Semantics

### Introduction

Syntax: the form or structure of the expressions, statements, and program units

Semantics: the meaning of the expressions, statements, and program units

Syntax and semantics provide a language's definition
- Users of a language definition
	- Other language designers
	- Implementers
	- Programmers (the users of the language)

> [!note]- Language's definition
> how the syntax and language are being defined

### The General Problem of Describing Syntax: Terminology

> [!note] John Backus' ALGOL
> it was a more formal data programming language than fortran
> 
> he established a terminology for processing syntaxes

A **sentence** is a string of characters over some
alphabet

A **language** is a set of sentences

A **lexeme** is the lowest level syntactic unit of a language (e.g., `*` , `sum`, `begin`)

A **token** is a category of lexemes (e.g., identifier)

> [!important]
> Lexeme and tokens are used interchangeably

### Formal Definition of Languages

Recognizers
- A recognition device reads input strings over the alphabet of the language and decides whether the input strings belong to the language
- Example: syntax analysis part of a compiler

Generators
- A device that generates sentences of a language
- One can determine if the syntax of a particular sentence is syntactically correct by comparing it to the structure of the generator

### BNF and Context-Free Grammars

Context-Free Grammars
- Developed by Noam Chomsky in the mid-1950s
- Language generators, meant to describe the syntax of natural languages
- Define a class of languages called context-free languages

> [!note]
> However, even an average computer scientist finds it hard to understand CFGs

Backus-Naur Form (1959)
- Invented by John Backus and Peter Naur to describe Algol 58
- BNF is equivalent to context-free grammars

### BNF Fundamentals

![](Pasted%20image%2020230907140434.png)

for example, the identifier could be
$$
\text{int} = \{A\mid B\mid C\}
$$
If we wanna define the assignment statement in java we can do
```
<assign> -> ...
```

### BNF Rules

An abstraction (or nonterminal symbol) can have more than one RHS
```
<stmt> -> <single_stmt> | begin <stmt_list> end
```
- [i] you can say that those enclosed in brackets in the RHS are variables

Describing lists
- Syntactic lists are described using **recursion**
	- those that can be placed in sequence
```
<ident_list> -> ident | ident, <ident_list>
```
- A derivation is a repeated application of rules, starting with the start symbol and ending with a sentence (all terminal symbols)

> [!example] Example of a derivation:

we're looking for an assignment statement to prove that $x = x+1$ is an assignment statement

```
<assign> -> <LHS> = <RHS>
			<id> =  <expr>
			A | B | C | x | v = <id> <operator> <const>
			                     x        +      1
			x = x + 1
```

![](Pasted%20image%2020230907142130.png)

### Derivations

![](Pasted%20image%2020230907142404.png)

### Parse Trees
- [i] hierarchical representation of derivation

# 09/09/2023

## Describing syntax and semantics

> [!note] Two ways of describing syntax
> Chomsky CFG
> BNF

### Backus-naur form symbols, conventions, components
- Nonterminal: $<>$
%% 	- an identifier that describes %%
- Implies/equivalent/equal: $\to$
- block delimiters: $\{\}$, `begin - end`
- stmt separator: $;$

### Grammar Ambiguity

a grammar that creates a sentential form with two or more distinct parse trees

![](Pasted%20image%2020230909183210.png)

![](Pasted%20image%2020230909175722.png)

## Extended BNF

> [!note] Page 130 or 151/816

Optional parts (0 or more) are placed  in brackets
```
<proc_call> -> ident [expr_list]
```

Alternative parts of the RHSs are placed inside parentheses and separated via vertical bars
```
<term> -> <term> (+|-) const
```

Repetitions (0 or more) are placed inside braces
```
<ident> -> letter {letter|digit}
```

# 09/16/2023

## Project Specifications

$$
\begin{matrix}\texttt{print(x)} & \to & \text{Syntax checker} & \to & \text{error or no error}\end{matrix}
$$
- specify the type of error at its presence

## Names, Bindings, Type checking, and Scope

Names
- features of a prog lang are the names that can be used for identifiers
	- issues
		- maximum length
		- legal connectors (e.g. - or \_)
		- case sensitivity
		- reserved words or keywords

Form of names
- usually alphanumeric (and connectors) with restrictions on the leading char
- some langauges have case sensitivity
- keywords - may be aliased
- reserved words - may not be aliased

> [!important]
> variables can't start with a number because it will be considered as a number

Variables
- abstraction of a memory location and the methods to perform operations
- aliases - multiple identifiers referring to the same memory location
- Type - specifies the allowable range of values to be stored and allowable operations

Binding
- association of an object to its attributes/operations/name
- binding time
	- time that the binding takes place
	- happens in design time, language implementation time, compile time, link time, load time, run time
- binding is static if it occurs before run time and remains unchanged
- binding is dynamic otherwise

Variable declarations
- explicit vs implicit
- dynamic type binding
	- binding of type is not explicit but derived by examining assignemnt statements at runtime (e.g. Lisp)
- Type inference
	- using inference rules to determine the type returned by a function

# 09/21/2023

## Chapter 4 Cont...

> [!quote] Not by sevesta

## Names and Binding

The basic idea behind procedural programming is storing the "State" of the process where the "State" is the current value of the variables stored in memory

In this and the next chapter we will consider the idea of variables, storage, binding, type, and implementation

> [!note] Binding variables
> we need to bind variables to their data type to know the scope of their input; i.e. assigning an integer to a variable means it can use 16-bits at most
> Contents

## Lifetimes

Time from which a variable is bound to memory until it is unbound

Static - bound from beginning to ending of execution
- Global variables are statically bound
- Allows variables in a subprog to retain values after subprog terminates but does not allow for recursion or shadowing
- Most efficient form of binding because of ability to perform direct addressing & no overhead for allocation/deallocation at runtime

![](Pasted%20image%2020230921135243.png)

> [!important] It will not adapt the stack dynamic if the variable is implicity assigned

![](Pasted%20image%2020230921135319.png)

![](Pasted%20image%2020230921135358.png)

## Type Checking

![](Pasted%20image%2020230921135412.png)

> [!example] Coercion example
> from an `int` to a `double` in an operation

A Programming language is strongly typed if [](Pasted%20image%2020230921135553.png)
- all type checking errors are detectable before run-time
- or, a more restrictive definition - every identifier in a program has a single type associated with it and known at compile time
- both require static binding
- ==before runtime==

![](Pasted%20image%2020230921135701.png)

## Type Compatibility

![](Pasted%20image%2020230921140002.png.md)

Name Type compatibility 
- if the vars are declared using the same declaration or the same type

Compatible by Structure 
- if the vars have the same structure even though they are of differently name types

## Scope

- The range of statements from which a variable is "visible" (referenced)
- If explicitly declared $\to$ local variable
- If explicitly passed to subprog $\to$ param
- If global $\to$ visible anywhere

![](Pasted%20image%2020230921140245.png)

![](Pasted%20image%2020230921140340.png)

![](Pasted%20image%2020230921140632.png)

![](Pasted%20image%2020230921140819.png)

![](Pasted%20image%2020230921140912.png)

![](Pasted%20image%2020230921141401.png)

## Referencing Environment

![](Pasted%20image%2020230921141724.png)

## Named Constants

![](Pasted%20image%2020230921142000.png)

## Variable Initialization

![](Pasted%20image%2020230921142255.png)

## Example

Variables/constants have the following attributes that might be bound to them
- data type
- name
- range of values
- value/s
- scope
- lifetime
- ref. env

all of them can happen at binding time
- compile time
- execution time
- language definition
- language design
- etc.

## Prelim Exam Coverage

- until the control statements (i.e. until chapter 4)

# 09/23/2023

## Dynamic Scoping

Pass by reference vs pass by value
- passing the memory address as to passing the value
- pass by reference is usually used by dynamic scoping

> [!important] For convenience, variable intialization can occur prior to execution

## Chapter 5: Data Types

> [!note]- Data type
> defines a collection of data values and a set of predefined operations on those values

- Every PL needs a variety of data types to better model/match the world
- more data types makes programming easier
	- [!] but too many data types might be confusing
- which data types are most common? which are necessary? which are uncommon yet useful?
- how are data types implemented in the PL

**Primitive data types**
- data types that could no longer be decomposed
	- there is only one single value that they can have
- supported directly in the hardware of the machine
- we also call them **simple data types**
- Integer
- Floating Point
- Decimal
- Boolean
- Character

**Character String Type**
- is one in which the values consist of sequences of characters

> [!warning]- Design Issues
> - Should strings be simply a special kind of character array or a primitive type?
> - Should strings have static or dynamic length?

> [!note] Compostie/Structured data types
> - arrays, records, etc.
> - having more than one value

### User-defined ordinal types

**ordinal type**
- the range of possible values can be easily associated with the positive integers
- e.g. integer, char, boolean

**enumeration type**
- all possible values are named constants
- each constant is called an *enumeration constant*
- enum constants are typically implicitly assigned the integer values: $0, 1, 2, ...$
	- [i] can always be explicitly assigned any integer literal in the types definition

### Subranges

- limiting a large type to a sequence of values within the larger range
- can only be used with orginal types
- provides additional flexibility in programming and readability/abstraction

### Array Types

**Design Issues**
- What types are legal for subscripts? 
- Are subscripting expressions in element references range checked? 
- When are subscript ranges bound? 
- When does array allocation take place? 
- Are ragged or rectangular multidimensioned arrays allowed, or both? 
- Can arrays be initialized when they have their storage allocated? 
- What kinds of slices are allowed, if any?

> [!note] Arrays are sometimes called ==finite mappings==

